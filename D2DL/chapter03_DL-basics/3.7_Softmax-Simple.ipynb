{"cells":[{"cell_type":"code","metadata":{"tags":[],"cell_id":"00000-3e7d3ac8-1fe4-42a0-99f0-923d2fc5b211","output_cleared":false,"source_hash":"7513c6b1","execution_millis":454,"execution_start":1605449695258},"source":"import torch\nfrom torch import nn\nfrom torch.nn import init\nimport numpy as np\nimport sys\nsys.path.append(\"..\") \nimport d2dl_library as d2l\n\nbatch_size = 256\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)","outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00001-f6d96525-52b9-4cc1-8e48-36a891676a9d","output_cleared":false,"source_hash":"32b8b0b6","execution_millis":2,"execution_start":1605449695716},"source":"num_inputs = 784\nnum_outputs = 10\n\nclass FlattenLayer(nn.Module):\n    def __init__(self):\n        super(FlattenLayer,self).__init__()\n    def forward(self,x):# x shape:(batch,*,*,...)\n        return x.view(x.shape[0],-1)\n","outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00002-1abf1f15-4509-4f2e-8a7d-e5e8be0abac5","output_cleared":false,"source_hash":"19f40f30","execution_millis":3,"execution_start":1605449695721},"source":"from collections import OrderedDict\nnet=nn.Sequential(\n    OrderedDict([\n        ('flatten',FlattenLayer()),\n        ('linear',nn.Linear(num_inputs,num_outputs))\n    ])\n)","outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00003-3f0a5d4a-6191-49b3-8c1f-8c0547156488","output_cleared":false,"source_hash":"16967780","execution_millis":4,"execution_start":1605449695726},"source":"init.normal_(net.linear.weight,mean=0,std=0.01)\ninit.constant_(net.linear.bias,val=0)","outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"Parameter containing:\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)"},"metadata":{}}],"execution_count":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00004-f02229d3-3018-401f-bb59-cbad0dd43c79","output_cleared":false,"source_hash":"fecc565e","execution_millis":45,"execution_start":1605449695733},"source":"loss=nn.CrossEntropyLoss()\noptimizer=torch.optim.SGD(net.parameters(),lr=0.1)","outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00005-8b1f4098-044c-4e2d-a4c5-487c96d03bc9","output_cleared":false,"source_hash":"8b3411f","execution_millis":33707,"execution_start":1605449695779},"source":"#模型net在数据集data_iter上的准确率\ndef evaluate_accuracy(data_iter,net):\n    acc_sum,n=0.0,0\n    for X,y in data_iter:\n        acc_sum+=(net(X).argmax(dim=1)==y).float().sum().item()\n        n+=y.shape[0]\n    return acc_sum/n\n\nnum_epochs=5\n\ndef train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,\n              params=None, lr=None, optimizer=None):\n    for epoch in range(num_epochs):\n        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n        for X, y in train_iter:\n            y_hat = net(X)\n            l = loss(y_hat, y).sum()\n            \n            # 梯度清零\n            if optimizer is not None:\n                optimizer.zero_grad()\n            elif params is not None and params[0].grad is not None:\n                for param in params:\n                    param.grad.data.zero_()\n            \n            l.backward()\n            if optimizer is None:\n                sgd(params, lr, batch_size)\n            else:\n                optimizer.step()  # “softmax回归的简洁实现”一节将用到\n            \n            \n            train_l_sum += l.item()\n            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n            n += y.shape[0]\n        test_acc = evaluate_accuracy(test_iter, net)\n        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\n\n\ntrain_ch3(net,train_iter,test_iter,loss,num_epochs,batch_size,None,None,optimizer)","outputs":[{"name":"stdout","text":"epoch 1, loss 0.0031, train acc 0.749, test acc 0.789\nepoch 2, loss 0.0022, train acc 0.813, test acc 0.804\nepoch 3, loss 0.0021, train acc 0.826, test acc 0.814\nepoch 4, loss 0.0020, train acc 0.832, test acc 0.817\nepoch 5, loss 0.0019, train acc 0.837, test acc 0.769\n","output_type":"stream"}],"execution_count":null}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"bfe02c7f-1644-49a9-8fd8-00636ba33497","deepnote_execution_queue":[]}}