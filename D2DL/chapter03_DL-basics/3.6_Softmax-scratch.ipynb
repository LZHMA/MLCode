{"cells":[{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00000-4c2cbfda-5580-4156-b0cb-9ef84079ba13","output_cleared":false,"source_hash":"935d9529","execution_millis":7701,"execution_start":1605397391914},"source":"import torch\nimport torchvision\nimport numpy as np\nimport sys\nsys.path.append(\"..\")\nimport d2dl_library as d2l\n\nbatch_size=256\ntrain_iter,test_iter=d2l.load_data_fashion_mnist(batch_size)","execution_count":null,"outputs":[{"name":"stdout","text":"Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /home/jovyan/Datasets/FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz\n100.0%Extracting /home/jovyan/Datasets/FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz to /home/jovyan/Datasets/FashionMNIST/FashionMNIST/raw\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /home/jovyan/Datasets/FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n111.0%Extracting /home/jovyan/Datasets/FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /home/jovyan/Datasets/FashionMNIST/FashionMNIST/raw\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /home/jovyan/Datasets/FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n100.0%Extracting /home/jovyan/Datasets/FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /home/jovyan/Datasets/FashionMNIST/FashionMNIST/raw\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /home/jovyan/Datasets/FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n159.1%Extracting /home/jovyan/Datasets/FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /home/jovyan/Datasets/FashionMNIST/FashionMNIST/raw\nProcessing...\nDone!\n/opt/venv/lib/python3.7/site-packages/torchvision/datasets/mnist.py:469: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00001-76a997f7-f916-42c4-a2e1-7d43d33fb55d","output_cleared":false,"source_hash":"3c423184","execution_millis":9,"execution_start":1605397399618},"source":"num_inputs = 784\nnum_outputs = 10\n\nW = torch.tensor(np.random.normal(0, 0.01, (num_inputs, num_outputs)), dtype=torch.float)\nb = torch.zeros(num_outputs, dtype=torch.float)\n\nW.requires_grad_(requires_grad=True)\nb.requires_grad_(requires_grad=True)\n\nprint(W.size())\n","execution_count":null,"outputs":[{"name":"stdout","text":"torch.Size([784, 10])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 实现softmax运算\n如何对多维`Tensor`按维度操作：对同一列（`dim=0`）或同一行（`dim=1`）的元素求和，并在结果中保留行和列这两个维度（`keepdim=True`）","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00002-cd3c2b87-2f1f-4243-a629-08e1071c7f0a","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00003-073525a3-ac06-4507-af2b-4d810c267679","output_cleared":false,"source_hash":"e2eed49e","execution_millis":0,"execution_start":1605397399627},"source":"def softmax(X):\n    X_exp=X.exp()\n    partition=X_exp.sum(dim=1,keepdim=True)\n    return X_exp/partition","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"可以看到，对于随机输入，我们将每个元素变成了非负数，且每一行和为1。","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00004-7536af55-ade7-4646-ac50-93e326d15d10","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00005-c19a3c7d-cb0a-4575-b748-0ea7495c7d41","output_cleared":false,"source_hash":"e3c17a7a","execution_millis":3,"execution_start":1605397399630},"source":"X=torch.rand((2,5))\nX_prob=softmax(X)\nprint(X_prob,X_prob.sum(dim=1))","execution_count":null,"outputs":[{"name":"stdout","text":"tensor([[0.1996, 0.2215, 0.1344, 0.2673, 0.1771],\n        [0.1344, 0.2543, 0.1307, 0.2636, 0.2169]]) tensor([1.0000, 1.0000])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 定义模型\n有了`softmax`运算，可以定义softmax回归模型了。这里通过`view`函数将每张原始图像改成长度为`num_input`的向量。","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00006-ff441b0b-2e86-43e0-959e-0c29f418dbbe","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00007-7af51b4c-c8df-47ea-bc88-2744b3462bf6","output_cleared":false,"source_hash":"4fb4c7d4","execution_millis":1,"execution_start":1605397399636},"source":"def net(X):\n    return softmax(torch.mm(X.view((-1, num_inputs)), W) + b)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 定义损失函数\n为了得到标签的预测概率，我们可以使用gather函数。在下面的例子中，变量y_hat是2个样本在3个类别的预测概率，变量y是这2个样本的标签类别。通过使用gather函数，我们得到了2个样本的标签的预测概率。与3.4节（softmax回归）数学表述中标签类别离散值从1开始逐一递增不同，在代码中，标签类别的离散值是从0开始逐一递增的。","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00008-d25ecbe5-25d5-4796-9303-c8cc5615042e","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00009-c3bde7e5-586d-4c20-8775-910c6c160902","output_cleared":false,"source_hash":"7a12d2","execution_millis":1,"execution_start":1605400504800},"source":"def cross_entropy(y_hat,y):\n    return -torch.log(y_hat.gather(1,y.view(-1,1)))","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 计算分类准确率\n给定一个类别的预测概率分布y_hat，我们把预测概率最大的类别作为输出类别。如果它与真实类别y一致，说明这次预测是正确的。分类准确率即正确预测数量与总预测数量之比。\n\n为了演示准确率的计算，下面定义准确率accuracy函数。其中y_hat.argmax(dim=1)返回矩阵y_hat每行中最大元素的索引，且返回结果与变量y形状相同。相等条件判断式(y_hat.argmax(dim=1) == y)是一个类型为ByteTensor的Tensor，我们用float()将其转换为值为0（相等为假）或1（相等为真）的浮点型Tensor。\n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00010-f98cbe14-0dbd-4788-9fa0-381b124f4fe3","output_cleared":false,"source_hash":"262be300","execution_start":1605357467682,"execution_millis":0}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00011-93e39643-8d14-4bf2-9047-3d37ede5a947","output_cleared":false,"source_hash":"30b71044","execution_millis":941,"execution_start":1605446053899},"source":"def accuracy(y_hat,y):\n    return (y_hat.argmax(dim=1)==y).float().mean().item()\n\n#模型net在数据集data_iter上的准确率\ndef evaluate_accuracy(data_iter,net):\n    acc_sum,n=0.0,0\n    for X,y in data_iter:\n        acc_sum+=(net(X).argmax(dim=1)==y).float().sum().item()\n        n+=y.shape[0]\n    return acc_sum/n\n\n#初始准确率应该接近0.1\nprint(evaluate_accuracy(test_iter,net))","execution_count":null,"outputs":[{"name":"stdout","text":"0.1499\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00012-9c2bf92d-f09f-40f1-b1f1-9da235743308","output_cleared":false,"source_hash":"e8d64f2f","execution_millis":274,"execution_start":1605447203586},"source":"num_epochs,lr=5,0.1\n\ndef train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,\n              params=None, lr=None, optimizer=None):\n    for epoch in range(num_epochs):\n        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n        for X, y in train_iter:\n            y_hat = net(X)\n            l = loss(y_hat, y).sum()\n\n            # 梯度清零\n            if optimizer is not None:\n                optimizer.zero_grad()\n            elif params is not None and params[0].grad is not None:\n                for param in params:\n                    param.grad.data.zero_()\n\n            l.backward()\n            if optimizer is None:\n                d2l.sgd(params, lr, batch_size)\n            else:\n                optimizer.step()  # “softmax回归的简洁实现”一节将用到\n\n\n            train_l_sum += l.item()\n            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n            n += y.shape[0]\n        test_acc = evaluate_accuracy(test_iter, net)\n        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\n\ntrain_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, batch_size, [W, b], lr)","execution_count":null,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"module 'd2dl_library' has no attribute 'sgd'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-16e099ea7312>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m               % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mtrain_ch3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-15-16e099ea7312>\u001b[0m in \u001b[0;36mtrain_ch3\u001b[0;34m(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr, optimizer)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                 \u001b[0md2l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# “softmax回归的简洁实现”一节将用到\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'd2dl_library' has no attribute 'sgd'"]}]}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"f327ebcf-877c-4f57-af28-ef224f65b929","deepnote_execution_queue":[]}}